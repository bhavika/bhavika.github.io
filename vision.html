<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title> Bhavika Tekwani </title>
  <meta name="description" content="Personal website, portfolio, bio">
  <meta name="author" content="Bhavika Tekwani">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Font Awesome Social Icons -->
  <link rel="stylesheet" href="./font-awesome-4.7.0/css/font-awesome.min.css">
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/bhavikatekwani.css">

</head>

<body>
  <div class="container">
    <div class="row">
      <div class="sixteen columns" style="margin-top: 40px">
        <h1>Bhavika Tekwani</h1>
        <h5>Technologist, also human.</h5>
      </div>

     <div class="sixteen columns">

      <h5> CS682: Computer Vision (Spring 2018)</h5>
      <h6> Assignment 1 </h6>

      <a href="http://bhavikatekwani.me/vision_hw1.html">Code</a>
      <a href="HW1_Vision.pdf"> Report </a>

      <table>
        <tr>
          <td colspan="2"> 1. Take a picture and convert it to grayscale. </td>
          <td> <img src="./vision_output/hw1/bhavika.png" alt="Bhavika" width="300" height="300"/> </td>
          <td> <img src="./vision_output/hw1/bhavika_grey.png" alt="Grayscale" width="300" height="300"/></td>
        </tr>

        <tr>
          <td> 2. Transformations </td>
          <td> Switch colorspaces </td>
          <td> <img src="./vision_output/hw1/bhavika_Luv.png" width="300" height="300"/> </td>
          <td> <img src="./vision_output/hw1/bhavika_grey_Luv.png" width="300" height="300"/> </td>
        </tr>

        <tr>
          <td> </td>
          <td> Perspective Transformation</td>
          <td> <img src="./vision_output/hw1/perspective_rgb.png" width="400" height="400"/> </td>
          <td> <img src="./vision_output/hw1/perspective_gray.png" width="400" height="400"/> </td>
        </tr>

        <tr>
          <td> </td>
          <td> Averaging </td>
          <td> <img src="./vision_output/hw1/averaging_rgb.png" width="300" height="300"/> </td>
          <td> <img src="./vision_output/hw1/averaging_grey.png" width="300" height="300"/> </td>
        </tr>

        <tr>
          <td> </td>
          <td> Median Blurring </td>
          <td> <img src="./vision_output/hw1/bhavika_rgb_median.png" width="300" height="300"/> </td>
          <td> <img src="./vision_output/hw1/bhavika_grey_grey_median.png" width="300" height="300"/> </td>
        </tr>

        <tr>
          <td> </td>
          <td> Blending </td>
          <td> <img src="./vision_output/hw1/blend.png" width="300" height="300"/> </td>
          <td></td>
        </tr>
    </table>

    <p> I have been observing the popularity of Mask R-CNN for object instance segmentation in the deep
        learning and vision community. </br>

        <a href="https://arxiv.org/abs/1703.06870">Original paper</a> </br>
        <a href="https://github.com/facebookresearch/Detectron">Original implementation</a> </br>
        <a href="https://github.com/matterport/Mask_RCNN">Alternative implementation</a> </br>

        Mask R-CNN is built upon the Feature Pyramid Network (FPN) and ResNet-101. It works by first
        proposing anchor boxes and then refining bounding boxes to identify the object in a particular box.
        Then the R-CNN generates masks to fit the shape of the identified object and overlays those
        masks onto the objects. </br>
        Mask R-CNN is known to achieve state of the art results on the COCO 2016 - Common Objects in
        Context dataset
    </p>



</body>
